# 17.1. 트랜스포머(Transformer)
- 트랜스포머
  - 2017년 구글 논문 "Attention is all you need"
  - 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션(Attention)만으로 구현한 모델
  - RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 성능도 RNN보다 우수함.

## 17.1.1. 기존의 seq2seq 모델의 한계
- 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실됨.
- 이 손실을 보정하기 위해 어텐션이 사용됨.

## 17.1.2. 트랜스포머(Transformer)의 주요 하이퍼파라미터
- 여기서는 트랜스포머에는 이러한 하이퍼파라미터가 존재한다는 정도로만 이해
- 아래 트랜스포머를 제안한 논문에서 사용한 수치값
- 하이퍼파라미터는 사용자가 모델 설계시 임의로 변경할 수 있는 값들

     ![image](https://user-images.githubusercontent.com/42113942/130957781-267d5f3f-18bc-4128-a75c-45d88c908344.png)
  
  - 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기 
  - 피드 포워드 신경망의 입력층과 출력층의 크기
  - 임베딩 벡터의 차원
  - 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때에도 이 차원을 유지함.

   ![image](https://user-images.githubusercontent.com/42113942/130958096-e59a6801-b030-46f1-8113-ee99f74f6255.png)
  
  -  트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지
  -  인코더와 디코더를 각각 총 6개
  
    ![image](https://user-images.githubusercontent.com/42113942/130958143-4f2c12ce-b88b-4276-b58b-953a554f1c2b.png)
  
  - 트랜스포머에서는 어텐션을 사용할 때, 1번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택함. 이때 이 병렬의 개수

   ![image](https://user-images.githubusercontent.com/42113942/130958192-5d5fdb95-7ba0-4d17-aace-e7d2ff98e580.png)
  
  - 피드 포워드 신경망의 은닉층의 크기

## 17.1.3. 트랜스포머(Transformer)

![image](https://user-images.githubusercontent.com/42113942/131215118-cb749a54-824f-45c8-9dbd-52e29cdc6b66.png)

![image](https://user-images.githubusercontent.com/42113942/131215113-5945fcb0-eb10-4616-b279-a1465d753de9.png)

- RNN을 사용하지 않지만 인코더-디코더 구조를 유지함.
- 다른 점은 인코더와 디코더라는 단위가 N개가 존재할 수 있음.
  - 이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time-step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조
  - 논문에서는 인코더와 디코더 각 6개 사용함(2번째 그림).

![image](https://user-images.githubusercontent.com/42113942/131215120-4b21660a-089f-443f-a6d1-17c9f2cbd218.png)

- 위 그림은 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 트랜스포머 구조를 보여줌.
- 디코더는 마치 기존의 seq2seq 구조처럼 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행함.
  - RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여줌.

- 이제 트랜스포머의 내부 구조를 조금씩 확대
  - 트랜스포머의 인코더와 디코더는 단순히 각 단어의 임베딩 벡터들을 입력받는 것이 아니라 임베딩 벡터에서 조정된 값을 입력받음. 
  - 입력 부분부터 확대
  
## 17.1.4. 포지셔널 인코딩(Positional Encoding)
