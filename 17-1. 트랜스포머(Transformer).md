### 17.1. 트랜스포머(Transformer)
- 트랜스포머
  - 2017년 구글 논문 "Attention is all you need"
  - 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션(Attention)만으로 구현한 모델
  - RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 성능도 RNN보다 우수함.

## 17.1.1. 기존의 seq2seq 모델의 한계
- 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실됨.
- 이 손실을 보정하기 위해 어텐션이 사용됨.

## 17.1.2. 트랜스포머(Transformer)의 주요 하이퍼파라미터
- 여기서는 트랜스포머에는 이러한 하이퍼파라미터가 존재한다는 정도로만 이해
- 아래 트랜스포머를 제안한 논문에서 사용한 수치값
- 하이퍼파라미터는 사용자가 모델 설계시 임의로 변경할 수 있는 값들

     ![image](https://user-images.githubusercontent.com/42113942/130957781-267d5f3f-18bc-4128-a75c-45d88c908344.png)
  
  - 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기 
  - 피드 포워드 신경망의 입력층과 출력층의 크기
  - 임베딩 벡터의 차원
  - 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때에도 이 차원을 유지함.

   ![image](https://user-images.githubusercontent.com/42113942/130958096-e59a6801-b030-46f1-8113-ee99f74f6255.png)
  
  -  트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지
  -  인코더와 디코더를 각각 총 6개
  
     ![image](https://user-images.githubusercontent.com/42113942/130958143-4f2c12ce-b88b-4276-b58b-953a554f1c2b.png)
  
  - 트랜스포머에서는 어텐션을 사용할 때, 1번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택함. 이때 이 병렬의 개수

   ![image](https://user-images.githubusercontent.com/42113942/130958192-5d5fdb95-7ba0-4d17-aace-e7d2ff98e580.png)
  
  - 피드 포워드 신경망의 은닉층의 크기

## 17.1.3. 트랜스포머(Transformer)
