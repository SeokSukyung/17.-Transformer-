### 17.1. 트랜스포머(Transformer)
- 트랜스포머
  - 2017년 구글 논문 "Attention is all you need"
  - 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션(Attention)만으로 구현한 모델
  - RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 성능도 RNN보다 우수함.

## 17.1.1. 기존의 seq2seq 모델의 한계
- 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실됨.
- 이 손실을 보정하기 위해 어텐션이 사용됨.

## 17.1.2. 트랜스포머(Transformer)의 주요 하이퍼파라미터
- 여기서는 트랜스포머에는 이러한 하이퍼파라미터가 존재한다는 정도로만 이해
- 아래 트랜스포머를 제안한 논문에서 사용한 수치값
- 하이퍼파라미터는 사용자가 모델 설계시 임의로 변경할 수 있는 값들

