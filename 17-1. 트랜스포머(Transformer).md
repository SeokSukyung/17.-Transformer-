# 17.1. 트랜스포머(Transformer)
- 트랜스포머
  - 2017년 구글 논문 "Attention is all you need"
  - 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션(Attention)만으로 구현한 모델
  - RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 성능도 RNN보다 우수함.

## 17.1.1. 기존의 seq2seq 모델의 한계
- 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실됨.
- 이 손실을 보정하기 위해 어텐션이 사용됨.

## 17.1.2. 트랜스포머(Transformer)의 주요 하이퍼파라미터
- 여기서는 트랜스포머에는 이러한 하이퍼파라미터가 존재한다는 정도로만 이해
- 아래 트랜스포머를 제안한 논문에서 사용한 수치값
- 하이퍼파라미터는 사용자가 모델 설계시 임의로 변경할 수 있는 값들

     ![image](https://user-images.githubusercontent.com/42113942/130957781-267d5f3f-18bc-4128-a75c-45d88c908344.png)
  
  - 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기 
  - 피드 포워드 신경망의 입력층과 출력층의 크기
  - 임베딩 벡터의 차원
  - 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때에도 이 차원을 유지함.

   ![image](https://user-images.githubusercontent.com/42113942/130958096-e59a6801-b030-46f1-8113-ee99f74f6255.png)
  
  -  트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지
  -  인코더와 디코더를 각각 총 6개
  
    ![image](https://user-images.githubusercontent.com/42113942/130958143-4f2c12ce-b88b-4276-b58b-953a554f1c2b.png)
  
  - 트랜스포머에서는 어텐션을 사용할 때, 1번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택함. 이때 이 병렬의 개수

   ![image](https://user-images.githubusercontent.com/42113942/130958192-5d5fdb95-7ba0-4d17-aace-e7d2ff98e580.png)
  
  - 피드 포워드 신경망의 은닉층의 크기

## 17.1.3. 트랜스포머(Transformer)

![image](https://user-images.githubusercontent.com/42113942/131215118-cb749a54-824f-45c8-9dbd-52e29cdc6b66.png)

![image](https://user-images.githubusercontent.com/42113942/131215113-5945fcb0-eb10-4616-b279-a1465d753de9.png)

- RNN을 사용하지 않지만 인코더-디코더 구조를 유지함.
- 다른 점은 인코더와 디코더라는 단위가 N개가 존재할 수 있음.
  - 이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time-step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조
  - 논문에서는 인코더와 디코더 각 6개 사용함(2번째 그림).

![image](https://user-images.githubusercontent.com/42113942/131215120-4b21660a-089f-443f-a6d1-17c9f2cbd218.png)

- 위 그림은 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 트랜스포머 구조를 보여줌.
- 디코더는 마치 기존의 seq2seq 구조처럼 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행함.
  - RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여줌.

- 이제 트랜스포머의 내부 구조를 조금씩 확대
  - 트랜스포머의 인코더와 디코더는 단순히 각 단어의 임베딩 벡터들을 입력받는 것이 아니라 임베딩 벡터에서 조정된 값을 입력받음. 
  - 입력 부분부터 확대
  
## 17.1.4. 포지셔널 인코딩(Positional Encoding)
- RNN은 단어를 순차적으로 입력받아서 처리하여 각 단어의 위치 정보(position information)를 가질 수 있음.
- 하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있음.
- 포지셔널 인코딩: 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는 것

![image](https://user-images.githubusercontent.com/42113942/131222740-2d1e3f30-f87e-4f5b-af83-c40c9b229fe1.png)

- 위 그림은 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩값이 더해지는 것을 보여줌.

- 아래 그림은 임베딩 벡터가 인코더의 입력으로 사용되기 전에 포지셔널 인코딩값이 더해지는 과정을 시각화
  
![image](https://user-images.githubusercontent.com/42113942/131222769-727638f6-661c-4472-be55-d6c05769d038.png)

- 포지셔널 인코딩의 값은 아래 두 함수를 사용하여 도출됨.

![image](https://user-images.githubusercontent.com/42113942/131222969-9c6f639e-dc2d-4486-ae65-b6b48f37ceb4.png)

![image](https://user-images.githubusercontent.com/42113942/131222973-8f62d73e-c8f5-4579-a534-6457d684a3ff.png)

- 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해주므로서 단어의 순서 정보를 더하여 줌. 
- pos, i, dmodel? 
- 위의 함수를 이해하기 위해서는 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 임베딩 벡터가 모여 만들어진 문장 벡터 행렬과 포지셔널 인코딩 행렬의 덧셈 연산을 통해 이루어진다는 점을 이해해야 함.

![image](https://user-images.githubusercontent.com/42113942/131223059-9b1eb569-e4f2-4f22-840f-7bf4ed4e4d98.png)

- pos: 입력 문장에서의 임베딩 벡터의 위치
- i: 임베딩 벡터 내의 차원의 인덱스

- 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우에는 사인 함수의 값을 사용하고 홀수인 경우에는 코사인 함수의 값을 사용함.
- dmodel: 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터
- 위 그림에서는 마치 4로 표현되었지만 실제 논문에서는 512의 값을 가짐.

- 위와 같은 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존됨.
- 각 임베딩 벡터에 포지셔널 인코딩값을 더하면 같은 단어라고 하더라도 문장 내의 위치에 따라서 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라짐.
- 트랜스포머의 입력은 순서 정보가 고려된 임베딩 벡터

- 코드

```
class PositionalEncoding(tf.keras.layers.Layer):
  def __init__(self, position, d_model):
    super(PositionalEncoding, self).__init__()
    self.pos_encoding = self.positional_encoding(position, d_model)

  def get_angles(self, position, i, d_model):
    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    return position * angles

  def positional_encoding(self, position, d_model):
    angle_rads = self.get_angles(
        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
        d_model=d_model)

    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용
    sines = tf.math.sin(angle_rads[:, 0::2])

    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용
    cosines = tf.math.cos(angle_rads[:, 1::2])

    angle_rads = np.zeros(angle_rads.shape)
    angle_rads[:, 0::2] = sines
    angle_rads[:, 1::2] = cosines
    pos_encoding = tf.constant(angle_rads)
    pos_encoding = pos_encoding[tf.newaxis, ...]

    print(pos_encoding.shape)
    return tf.cast(pos_encoding, tf.float32)

  def call(self, inputs):
    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]
```

- 50 × 128의 크기를 가지는 포지셔널 인코딩 행렬을 시각화
  - 입력 문장의 단어가 50개이면서, 각 단어가 128차원의 임베딩 벡터를 가질 때 사용할 수 있는 행렬
  
```
# 문장의 길이 50, 임베딩 벡터의 차원 128
sample_pos_encoding = PositionalEncoding(50, 128)

plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')
plt.xlabel('Depth')
plt.xlim((0, 128))
plt.ylabel('Position')
plt.colorbar()
plt.show()
```
```
(1, 50, 128) 
```  

![image](https://user-images.githubusercontent.com/42113942/131223296-1508ce3a-6e86-4d77-8d10-0e71eddf0702.png)

## 17.1.5. 어텐션(Attention)
- 트랜스포머에서 사용되는 세 가지의 어텐션

![image](https://user-images.githubusercontent.com/42113942/131223313-141a3d38-3919-4670-9d2a-742f94124bf5.png)

- 1) Encoder Self-Attention: 인코더에서 이루어짐. 본질적으로 Query, Key, Value가 동일한 경우
- 2) Masked Decoder Self-Attention: 디코더에서 이루어짐.
- 3) Encoder-Decoder Attention: 디코더에서 이루어짐. Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않음.
  - 주의할 점은 여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아니라 벡터의 출처가 같다는 의미

- 정리하면 다음과 같음.
``` 
인코더의 셀프 어텐션 : Query = Key = Value
디코더의 마스크드 셀프 어텐션 : Query = Key = Value
디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터
``` 
  
![image](https://user-images.githubusercontent.com/42113942/131223384-5d429ef3-303e-4a17-805c-5554dd5f677f.png)

- 위 그림은 트랜스포머의 아키텍처에서 세 가지 어텐션이 각각 어디에서 이루어지는지를 보여줌.
- 세 개의 어텐션에 추가적으로 '멀티 헤드'라는 이름이 붙어있는데, 이는 트랜스포머가 어텐션을 병렬적으로 수행하는 방법을 의미함(뒤에서 설명).
  
## 17.1.6. 인코더(Encoder)
- 인코더의 구조

![image](https://user-images.githubusercontent.com/42113942/131223402-22136f6d-ba7d-4dc1-96bf-ef8dd29d072f.png)

- 트랜스포머는 하이퍼파라미터인 num_layers 개수의 인코더 층을 쌓음.
  - 논문에서는 총 6개 인코더 층 사용
- 인코더를 하나의 층이라는 개념으로 생각한다면, 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어짐. => 셀프 어텐션과 피드 포워드 신경망
- 멀티 헤드 셀프 어텐션은 셀프 어텐션을 병렬적으로 사용하였다는 의미
- 포지션 와이즈 피드 포워드 신경망은 우리가 알고있는 일반적인 피드 포워드 신경망
  
## 17.1.7. 인코더의 셀프 어텐션
- 앞서 배웠던 어텐션 함수에 대해서 복습하고, 셀프 어텐션이 앞서 배웠던 어텐션과 무엇이 다른지 이해함.

1) 셀프 어텐션의 의미와 이점
