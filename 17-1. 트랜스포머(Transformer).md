# 17.1. 트랜스포머(Transformer)
- 트랜스포머
  - 2017년 구글 논문 "Attention is all you need"
  - 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션(Attention)만으로 구현한 모델
  - RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 성능도 RNN보다 우수함.

## 17.1.1. 기존의 seq2seq 모델의 한계
- 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실됨.
- 이 손실을 보정하기 위해 어텐션이 사용됨.
- 트랜스포머는 어텐션을 RNN의 보정을 위한 용도로 사용하는 것이 아니라 아예 어텐션으로 인코더와 디코더를 만들어보는 것

## 17.1.2. 트랜스포머(Transformer)의 주요 하이퍼파라미터
- 여기서는 트랜스포머에는 이러한 하이퍼파라미터가 존재한다는 정도로만 이해
- 하이퍼파라미터는 사용자가 모델 설계시 임의로 변경할 수 있는 값들로 아래 트랜스포머를 제안한 논문에서 사용한 수치값임.

     ![image](https://user-images.githubusercontent.com/42113942/130957781-267d5f3f-18bc-4128-a75c-45d88c908344.png)
  
  - 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기 
  - 피드 포워드 신경망의 입력층과 출력층의 크기
  - 임베딩 벡터의 차원
  - 각 인코더와 디코더가 다음 층의 인코더와 디코더로 값을 보낼 때에도 이 차원을 유지함.

   ![image](https://user-images.githubusercontent.com/42113942/130958096-e59a6801-b030-46f1-8113-ee99f74f6255.png)
  
  -  트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지
  -  인코더와 디코더를 각각 총 6개
  
    ![image](https://user-images.githubusercontent.com/42113942/130958143-4f2c12ce-b88b-4276-b58b-953a554f1c2b.png)
  
  - 트랜스포머에서는 어텐션을 사용할 때, 1번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값을 다시 하나로 합치는 방식을 택함. 이때 이 병렬의 개수

   ![image](https://user-images.githubusercontent.com/42113942/130958192-5d5fdb95-7ba0-4d17-aace-e7d2ff98e580.png)
  
  - 피드 포워드 신경망의 은닉층의 크기

## 17.1.3. 트랜스포머(Transformer)

![image](https://user-images.githubusercontent.com/42113942/131215118-cb749a54-824f-45c8-9dbd-52e29cdc6b66.png)

![image](https://user-images.githubusercontent.com/42113942/131215113-5945fcb0-eb10-4616-b279-a1465d753de9.png)

- RNN을 사용하지 않지만 인코더-디코더 구조를 유지함.
- 다른 점은 인코더와 디코더라는 단위가 N개가 존재할 수 있음.
  - 이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time-step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조
  - 논문에서는 인코더와 디코더 각 6개 사용함(위 2번째 그림).

[비교] RNN을 사용한 어텐션 메커니즘

![image](https://user-images.githubusercontent.com/42113942/131273295-e032f61e-0c67-436d-8873-9b23218745dc.png)

![image](https://user-images.githubusercontent.com/42113942/131215120-4b21660a-089f-443f-a6d1-17c9f2cbd218.png)

- 위 그림은 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 트랜스포머 구조를 보여줌.
- 디코더는 마치 기존의 seq2seq 구조처럼 시작 심볼 sos를 입력으로 받아 종료 심볼 eos가 나올 때까지 연산을 진행함.
  - RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여줌.

- 이제 트랜스포머의 내부 구조를 조금씩 확대
  - 트랜스포머의 인코더와 디코더는 단순히 각 단어의 임베딩 벡터들을 입력받는 것이 아니라 임베딩 벡터에서 조정된 값을 입력받음. 
  - 입력 부분부터 확대
  
## 17.1.4. 포지셔널 인코딩(Positional Encoding)
- RNN은 단어를 순차적으로 입력받아서 처리하여 각 단어의 위치 정보(position information)를 가질 수 있음.
- 하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있음.
- 포지셔널 인코딩: 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는 것

![image](https://user-images.githubusercontent.com/42113942/131222740-2d1e3f30-f87e-4f5b-af83-c40c9b229fe1.png)

- 위 그림은 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩값이 더해지는 것을 보여줌.

- 아래 그림은 임베딩 벡터가 인코더의 입력으로 사용되기 전에 포지셔널 인코딩값이 더해지는 과정을 시각화
  
![image](https://user-images.githubusercontent.com/42113942/131222769-727638f6-661c-4472-be55-d6c05769d038.png)

- 포지셔널 인코딩의 값은 아래 두 함수를 사용하여 도출됨.

![image](https://user-images.githubusercontent.com/42113942/131222969-9c6f639e-dc2d-4486-ae65-b6b48f37ceb4.png)

![image](https://user-images.githubusercontent.com/42113942/131222973-8f62d73e-c8f5-4579-a534-6457d684a3ff.png)

- 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더하여 단어의 순서 정보를 더함. 
- pos, i, dmodel? 
- 위의 함수를 이해하기 위해서는 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 임베딩 벡터가 모여 만들어진 문장 벡터 행렬과 포지셔널 인코딩 행렬의 덧셈 연산을 통해 이루어진다는 점을 이해해야 함.

![image](https://user-images.githubusercontent.com/42113942/131223059-9b1eb569-e4f2-4f22-840f-7bf4ed4e4d98.png)

- pos: 입력 문장에서의 임베딩 벡터의 위치
- i: 임베딩 벡터 내의 차원의 인덱스; 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우에는 사인 함수의 값을 사용하고 홀수인 경우에는 코사인 함수의 값을 사용함.
- dmodel: 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터; 위 그림에서는 마치 4로 표현되었지만 실제 논문에서는 512의 값을 가짐.

- 위와 같은 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존됨.
- 각 임베딩 벡터에 포지셔널 인코딩값을 더하면 같은 단어라고 하더라도 문장 내의 위치에 따라서 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라짐 => 트랜스포머의 입력은 순서 정보가 고려된 임베딩 벡터

- 포지셔널 인코딩에 사인 함수, 코사인 함수를 사용하는 이유? (https://yngie-c.github.io/nlp/2020/07/01/nlp_transformer/ 참고)
  - 포지셔널 인코딩의 조건: 1)각 위치마다 유일한 값을 출력해야 함(두 단어의 위치가 같을 수는 없음). 2) 길이가 다른 문장의 단어 위치를 나타낼 때에도 단어의 상대적인 거리가 같으면 같은 차이를 보여야 함. 

[예시]
```
“어머님 나 는 별 하나 에 아름다운 말 한마디 씩 불러 봅니다”

“소학교 때 책상 을 같이 했 던 아이 들 의 이름 과 패 경 옥 이런 이국 소녀 들 의 이름 과 벌써 애기 어머니 된 계집애 들 의 이름 과 가난 한 이웃 사람 들 의 이름 과 비둘기 강아지 토끼 노새 노루 프란시스 쟘 라이너 마리아 릴케 이런 시인 의 이름 을 불러 봅니다”
```
- 첫 번째 문장에서 “나”와 “아름다운”은 사이에 4개의 형태소를 두고 있고, 마찬가지로 두 번째 문장에서도 “책상”과 “아이”는 4개의 형태소를 사이에 두고 떨어져 있음.

1) “1, 2, 3, 4 …” 처럼 단어 순서대로 고유한 값을 부여할 경우
```
1: ‘어머님’, 2: ‘나’, 3: ‘는’, 4: ‘별’, 5: ‘하나’, 6: ‘에’, 7: ‘아름다운’, 8: ‘말’, 9: ‘한마디’, 10: ‘씩’, 11: ‘불러’, 12: ‘봅니다’

1: ‘소학교’, 2: ‘때’, 3: ‘책상’, 4: ‘을’, 5: ‘같이’, 6: ‘했’, 7: ‘던’, 8: ‘아이’, 9: ‘들’, 10: ‘의’, 11: ‘이름’, 12: ‘과’, 13: ‘패’, 14: ‘경’, 15: ‘옥’, 16: ‘이런’, 17: ‘이국’, 18: ‘소녀’, 19: ‘들’, 20: ‘의’, 21: ‘이름’, 22: ‘과’, 23: ‘벌써’, 24: ‘애기’, 25: ‘어머니’, 26: ‘된’, 27: ‘계집애’, 28: ‘들’, 29: ‘의’, 30: ‘이름’, 31: ‘과’, 32: ‘가난’, 33: ‘한’, 34: ‘이웃’, 35: ‘사람’, 36: ‘들’, 37: ‘의’, 38: ‘이름’, 39: ‘과’, 40: ‘비둘기’, 41: ‘강아지’, 42: ‘토끼’, 43: ‘노새’, 44: ‘노루’, 45: ‘프란시스’, 46: ‘쟘’, 47: ‘라이너’, 48: ‘마리아’, 49: ‘릴케’, 50: ‘이런’, 51: ‘시인’, 52: ‘의’, 53: ‘이름’, 54: ‘을’, 55: ‘불러’, 56: ‘봅니다’
```
  - 위와 같이 인코딩 값을 부여하게 되면 긴 문장에서 맨 뒤에 위치한 토큰의 값이 매우 커짐. 
  - 포지셔널 인코딩 값이 매우 커지게 되면 원래의 인풋 임베딩 값에 영향을 주게 됨.

2) [0,1]로 범위를 정해놓고 등분할 경우
```
0: ‘어머님’, 0.091: ‘나’, 0.182: ‘는’, 0.273: ‘별’, 0.364: ‘하나’, 0.455: ‘에’, 0.545: ‘아름다운’, 0.636: ‘말’, 0.727: ‘한마디’, 0.818: ‘씩’, 0.909: ‘불러’, 1: ‘봅니다’

0: ‘소학교’, 0.018: ‘때’, 0.036: ‘책상’, 0.055: ‘을’, 0.073: ‘같이’, 0.091: ‘했’, 0.109: ‘던’, 0.127: ‘아이’, 0.145: ‘들’, 0.164: ‘의’, 0.182: ‘이름’, 0.2: ‘과’, 0.218: ‘패’, 0.236: ‘경’, 0.255: ‘옥’, 0.273: ‘이런’, 0.291: ‘이국’, 0.309: ‘소녀’, 0.327: ‘들’, 0.345: ‘의’, 0.364: ‘이름’, 0.382: ‘과’, 0.4: ‘벌써’, 0.418: ‘애기’, 0.436: ‘어머니’, 0.455: ‘된’, 0.473: ‘계집애’, 0.491: ‘들’, 0.509: ‘의’, 0.527: ‘이름’, 0.545: ‘과’, 0.564: ‘가난’, 0.582: ‘한’, 0.6: ‘이웃’, 0.618: ‘사람’, 0.636: ‘들’, 0.655: ‘의’, 0.673: ‘이름’, 0.691: ‘과’, 0.709: ‘비둘기’, 0.727: ‘강아지’, 0.745: ‘토끼’, 0.764: ‘노새’, 0.782: ‘노루’, 0.8: ‘프란시스’, 0.818: ‘쟘’, 0.836: ‘라이너’, 0.855: ‘마리아’, 0.873: ‘릴케’, 0.891: ‘이런’, 0.909: ‘시인’, 0.927: ‘의’, 0.945: ‘이름’, 0.964: ‘을’, 0.982: ‘불러’, 1: ‘봅니다’
```
  - 첫 번째 문장에서 “어머님”과 “나”, 두 번째 문장에서 “소학교”와 “때”는 모두 바로 옆 단어이지만 첫 번째 문장에서의 차이는 0.091이고 두 번째 문장에서는 0.018으로 5배나 차이남.

3) 사인 함수, 코사인 함수를 사용할 경우 논문에서는 이런 조건을 모두 만족함.

![image](https://user-images.githubusercontent.com/42113942/131277573-2b2509ba-b5ba-4a17-a428-8b386ddce15a.png)
 
  - 아래의 그림은 포지셔널 인코딩 함수를 사용하여 사용하여 포지셔널 인코딩한 단어 사이의 거리를 시각화한 것. 
  - 왼쪽 그래프는 10개의 단어로 이루어진 문장의 모든 단어 사이의 거리를, 오른쪽 그래프는 50개의 단어로 이루어진 문장에서 앞부분 10개의 단어 사이의 거리를 시각화

![image](https://user-images.githubusercontent.com/42113942/131277624-8389de50-39e5-450d-a926-b7d3523a884e.png)

  - 두 그래프로부터 문장의 길이가 달라지더라도 단어 사이의 거리는 보존됨: 문장의 단어가 몇 개이든 항상 바로 옆 단어는 3.714만큼, 그 옆 단어는 6.967만큼 차이가 나게 됨.
  - 게다가 문장의 길이가 길어져도 포지셔널 인코딩 값이 한없이 커지지 않음: 멀리 떨어질수록 증가폭이 점점 떨어지기 때문에 “W1”과 “W2”사이의 거리는 3이상이지만, “W1”과 “W10”사이의 거리는 12.37로 10배가 아니라 4배 정도의 차이가 남.


- 코드

```
class PositionalEncoding(tf.keras.layers.Layer):
  def __init__(self, position, d_model):
    super(PositionalEncoding, self).__init__()
    self.pos_encoding = self.positional_encoding(position, d_model)

  def get_angles(self, position, i, d_model):
    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    return position * angles

  def positional_encoding(self, position, d_model):
    angle_rads = self.get_angles(
        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
        d_model=d_model)

    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용
    sines = tf.math.sin(angle_rads[:, 0::2])

    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용
    cosines = tf.math.cos(angle_rads[:, 1::2])

    angle_rads = np.zeros(angle_rads.shape)
    angle_rads[:, 0::2] = sines
    angle_rads[:, 1::2] = cosines
    pos_encoding = tf.constant(angle_rads)
    pos_encoding = pos_encoding[tf.newaxis, ...]

    print(pos_encoding.shape)
    return tf.cast(pos_encoding, tf.float32)

  def call(self, inputs):
    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]
```

- 50 × 128의 크기를 가지는 포지셔널 인코딩 행렬을 시각화
  - 입력 문장의 단어가 50개이면서, 각 단어가 128차원의 임베딩 벡터를 가질 때 사용할 수 있는 행렬
  
```
# 문장의 길이 50, 임베딩 벡터의 차원 128
sample_pos_encoding = PositionalEncoding(50, 128)

plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')
plt.xlabel('Depth')
plt.xlim((0, 128))
plt.ylabel('Position')
plt.colorbar()
plt.show()
```
```
(1, 50, 128) 
```  

![image](https://user-images.githubusercontent.com/42113942/131223296-1508ce3a-6e86-4d77-8d10-0e71eddf0702.png)

## 17.1.5. 어텐션(Attention)
- 트랜스포머에서 사용되는 세 가지의 어텐션

![image](https://user-images.githubusercontent.com/42113942/131223313-141a3d38-3919-4670-9d2a-742f94124bf5.png)

- 1) Encoder Self-Attention: 인코더에서 이루어짐. 본질적으로 Query, Key, Value가 동일한 경우
- 2) Masked Decoder Self-Attention: 디코더에서 이루어짐.
- 3) Encoder-Decoder Attention: 디코더에서 이루어짐. Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않음.
  - 주의할 점은 여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아니라 벡터의 출처가 같다는 의미

- 정리하면 다음과 같음.
``` 
인코더의 셀프 어텐션 : Query = Key = Value
디코더의 마스크드 셀프 어텐션 : Query = Key = Value
디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터
``` 
  
![image](https://user-images.githubusercontent.com/42113942/131223384-5d429ef3-303e-4a17-805c-5554dd5f677f.png)

- 위 그림은 트랜스포머의 아키텍처에서 세 가지 어텐션이 각각 어디에서 이루어지는지를 보여줌.
- 세 개의 어텐션에 추가적으로 '멀티 헤드'라는 이름이 붙어있는데, 이는 트랜스포머가 어텐션을 병렬적으로 수행하는 방법을 의미함(뒤에서 설명).
  
## 17.1.6. 인코더(Encoder)
- 인코더의 구조

![image](https://user-images.githubusercontent.com/42113942/131223402-22136f6d-ba7d-4dc1-96bf-ef8dd29d072f.png)

- 트랜스포머는 하이퍼파라미터인 num_layers 개수의 인코더 층을 쌓음.
  - 논문에서는 총 6개 인코더 층 사용
- 인코더를 하나의 층이라는 개념으로 생각한다면, 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어짐. => 셀프 어텐션과 피드 포워드 신경망
- 멀티 헤드 셀프 어텐션은 셀프 어텐션을 병렬적으로 사용하였다는 의미
- 포지션 와이즈 피드 포워드 신경망은 우리가 알고있는 일반적인 피드 포워드 신경망
  
## 17.1.7. 인코더의 셀프 어텐션
- 앞서 배웠던 어텐션 함수에 대해서 복습하고, 셀프 어텐션이 앞서 배웠던 어텐션과 무엇이 다른지 이해함.

1) 셀프 어텐션의 의미와 이점
- 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구함.
- 구해낸 이 유사도를 가중치로 하여 키와 맵핑되어있는 각각의 '값(Value)'에 반영함.
- 유사도가 반영된 '값(Value)'을 모두 가중합하여 리턴함.
  
![image](https://user-images.githubusercontent.com/42113942/131238710-4ba5cc09-b985-4662-bfcd-b1e69a6a33e5.png)

- 셀프 어텐션(self-attention)은 어텐션을 자기 자신에게 수행한다는 의미임.

- seq2seq에서 어텐션을 사용할 경우의 Q, K, V의 정의
```
Q = Query : t 시점의 디코더 셀에서의 은닉 상태
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
```  
- 그런데 사실 t 시점이라는 것은 계속 변화하면서 반복적으로 쿼리를 수행하므로 결국 전체 시점에 대해서 일반화를 할 수 있음.
```  
Q = Querys : 모든 시점의 디코더 셀에서의 은닉 상태들
K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
V = Values : 모든 시점의 인코더 셀의 은닉 상태들
```  
- 기존에는 디코더 셀의 은닉 상태가 Q이고 인코더 셀의 은닉 상태가 K라는 점에서 Q와 K가 서로 다른 값을 가지고 있었음. 그런데 셀프 어텐션에서는 Q, K, V가 전부 동일함.
```  
Q : 입력 문장의 모든 단어 벡터들
K : 입력 문장의 모든 단어 벡터들
V : 입력 문장의 모든 단어 벡터들
```  
-  셀프 어텐션을 통해 얻을 수 있는 대표적인 효과

![image](https://user-images.githubusercontent.com/42113942/131238767-5e026356-0a4a-4339-b2d0-af68e6241e0e.png)

- '그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.'
- 그것(it)은 길(street)? 동물(animal)?
- 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구하므로서 그것(it)이 동물(animal)과 연관되었을 확률이 높다는 것을 찾아냄.

- 트랜스포머에서의 셀프 어텐션의 동작 메커니즘

2) Q, K, V 벡터 얻기
- 우선 각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침.
- 이 Q벡터, K벡터, V벡터들은 초기 입력인 dmodel의 차원을 가지는 단어 벡터들보다 더 작은 차원을 가짐.
- dmodel=512차원, Q벡터, K벡터, V벡터=64차원
  
- 64는 트랜스포머의 또 다른 하이퍼파라미터인 num_heads로 결정됨.
- dmodel을 num_heads로 나눈 값을 각 Q벡터, K벡터, V벡터의 차원으로 결정함.
- 논문에서는 num_heads=8
- 아래 예시는 student라는 단어 벡터를 Q, K, V벡터로 변환하는 과정

![image](https://user-images.githubusercontent.com/42113942/131238898-3850404e-dff0-461e-999f-7af70441d9d4.png)

- 기존의 벡터로부터 더 작은 벡터는 가중치 행렬을 곱하므로서 완성됨.
- 각 가중치 행렬은 dmodel X (dmodel/num_heads)의 크기를 가짐.
- 이 가중치 행렬은 훈련 과정에서 학습됨.
- 논문에서는 dmodel=512, num_heads=8이므로 64의 크기를 가지는 Q, K, V벡터를 얻음.
- 모든 단어 벡터에 위와 같은 과정을 거치면 I, am, a, student는 각각의 Q, K, V 벡터를 얻음.
  
3) 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)
- Q, K, V 벡터를 얻었다면 지금부터는 기존에 배운 어텐션 메커니즘과 동일함.
- 각 Q 벡터는 모든 K 벡터에 대해서 어텐션 스코어를 구하고, 어텐션 분포를 구한 뒤에 이를 사용하여 모든 V 벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구하고, 이를 모든 Q 벡터에 대해서 반복함.
  
- 어텐션 챕터에 사용했던 내적만을 사용하는 어텐션 함수 
  ![image](https://user-images.githubusercontent.com/42113942/131239046-5faddbab-df9c-4cd7-a300-57ef5132a262.png)

- 위 값을 특정값으로 나눠준 어텐션 함수 => 트랜스포머에서 사용하는 어텐션 함수
  ![image](https://user-images.githubusercontent.com/42113942/131239078-24a51d62-8296-41ab-9e55-87fb2a77c2d5.png)
  
- 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention): 닷-프로덕트 어텐션(dot-product attention)에서 값을 스케일링하는 것을 추가하였다는 의미
  
![image](https://user-images.githubusercontent.com/42113942/131239081-ae3f0ff0-2ee1-4f8c-9044-00d97f693e6e.png)

- 우선 단어 I에 대한 Q벡터를 기준으로 설명
- 지금부터 설명하는 과정은 am에 대한 Q벡터, a에 대한 Q벡터, student에 대한 Q벡터에 대해서도 모두 동일한 과정을 거침. 
  
- 위의 그림은 단어 I에 대한 Q 벡터가 모든 K 벡터에 대해서 어텐션 스코어를 구하는 것을 보여줌.
- 위의 128과 32는 저자가 임의로 가정한 수치임.
  
- 위의 그림에서 어텐션 스코어는 각각 단어 I가 단어 I, am, a, student와 얼마나 연관되어 있는지를 보여주는 수치
- 트랜스포머에서는 두 벡터의 내적값을 스케일링하는 값으로 다음을 사용함.
  ![image](https://user-images.githubusercontent.com/42113942/131239162-cd346414-a4fe-4832-a5d1-4823aa2bd832.png)

  ![image](https://user-images.githubusercontent.com/42113942/131239179-49c63699-4c66-41de-b859-134269a43fc9.png)
  
![image](https://user-images.githubusercontent.com/42113942/131239183-e734abfa-88d2-4412-8db0-2145299b3535.png)

- 이제 어텐션 스코어에 소프트맥스 함수를 사용하여 어텐션 분포(Attention Distribution)을 구하고, 각 V 벡터와 가중합하여 어텐션 값(Attention Value)을 구함.
- 이를 단어 I에 대한 어텐션 값 또는 단어 I에 대한 컨텍스트 벡터(context vector)라고도 부름.
- 이제 am에 대한 Q벡터, a에 대 Q벡터, student에 대한 Q벡터에 대해서도 모두 동일한 과정을 반복하여 각각에 대한 어텐션 값을 구함.
  
- 그런데 굳이 이렇게 각 Q 벡터마다 일일히 따로 연산할 필요가 있을까?

4) 행렬 연산으로 일괄 처리하기
- 위의 과정들은 벡터 연산이 아니라 행렬 연산을 사용하면 일괄 계산이 가능함.
- 실제로는 행렬 연산으로 구현됨.
  
- 각 단어 벡터마다 일일히 가중치 행렬을 곱하는 것이 아니라 문장 행렬에 가중치 행렬을 곱하여 Q 행렬, K 행렬, V행렬을 구함.
  
![image](https://user-images.githubusercontent.com/42113942/131239243-a695aeda-0a29-49fe-9bba-f638950ed7ae.png)

- 이제 행렬 연산을 통해 어텐션 스코어는 구하는 방법
- 여기서 Q 행렬을 K 행렬을 전치한 행렬과 곱하면 각각의 단어의 Q벡터와 K벡터의 내적이 각 행렬의 원소가 되는 행렬이 결과로 나옴.
  
![image](https://user-images.githubusercontent.com/42113942/131239257-98ecce46-5b94-4d2f-bbf0-1f40d7e7e35b.png)

- 다시 말해 위의 그림의 결과 행렬의 값에 전체적으로 루트 dk를 나누어주면 이는 각 행과 열이 어텐션 스코어값을 가지는 행렬이 됨.
  - I 행과 student 열의 값은 I의 Q벡터와 student의 K 벡터의 어텐션 스코어와 동일한 행렬이 된다는 것

- 어텐션 스코어 행렬을 구하였다면 남은 것은 어텐션 분포를 구하고, 이를 사용하여 모든 단어에 대한 어텐션 값을 구하기
  - 이는 간단하게 어텐션 스코어 행렬에 소프트맥스 함수를 사용하고, V 행렬을 곱하는 것으로 해결됨.
  - 이렇게 되면 각 단어의 어텐션 값을 모두 가지는 어텐션 값 행렬이 결과로 나옴.

![image](https://user-images.githubusercontent.com/42113942/131239287-7d07545e-ef6e-4590-91f5-362aa8424f74.png)

- 위의 그림은 행렬 연산을 통해 모든 값이 일괄 계산되는 과정을 식으로 보여줌.
  - 실제 트랜스포머 논문에 기재된 아래의 수식과 정확하게 일치하는 식
  
  ![image](https://user-images.githubusercontent.com/42113942/131239339-600450d9-b106-4eb2-900f-e681fe6adf7a.png)

- 위 행렬 연산에 사용된 행렬의 크기
  - 입력 문장의 길이 = seq_len, Q, K 벡터 크기 = dk, V 벡터 크기: dv
  - 문장 행렬의 크기 = (seq_len, dmodel)
  - Q, K 행렬의 크기 = (seq_len, dk); V 행렬의 크기 = (seq_len, dv)
  - WQ, Wk = (dmodel, dk); Wv = (dmodel, dv)
  - 어탠션 값 행렬 a의 크기 = (seq_len, dv)
  
5) 스케일드 닷-프로덕트 어텐션 구현하기
``` 
def scaled_dot_product_attention(query, key, value, mask):
  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)
  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)
  # padding_mask : (batch_size, 1, 1, key의 문장 길이)

  # Q와 K의 곱. 어텐션 스코어 행렬.
  matmul_qk = tf.matmul(query, key, transpose_b=True)

  # 스케일링
  # dk의 루트값으로 나눠준다.
  depth = tf.cast(tf.shape(key)[-1], tf.float32)
  logits = matmul_qk / tf.math.sqrt(depth)

  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.
  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.
  if mask is not None:
    logits += (mask * -1e9)

  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.
  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)
  attention_weights = tf.nn.softmax(logits, axis=-1)

  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
  output = tf.matmul(attention_weights, value)

  return output, attention_weights
```
- Q 행렬과 K 행렬을 전치한 행렬을 곱하고, 소프트맥스 함수를 사용하여 어텐션 분포 행렬을 얻은 뒤에 V 행렬과 곱함.
- mask가 사용되는 if문은 아직 배우지 않은 내용으로 지금은 무시하고 넘어감.

- scaled_dot_product_attention 함수가 정상 작동하는지 테스트 temp_q, temp_k, temp_v라는 임의의 Query, Key, Value 행렬을 만들고, 이를 scaled_dot_product_attention 함수에 입력으로 넣어 함수가 리턴하는 값을 출력해볼 것

```
# 임의의 Query, Key, Value인 Q, K, V 행렬 생성
np.set_printoptions(suppress=True)
temp_k = tf.constant([[10,0,0],
                      [0,10,0],
                      [0,0,10],
                      [0,0,10]], dtype=tf.float32)  # (4, 3)

temp_v = tf.constant([[   1,0],
                      [  10,0],
                      [ 100,5],
                      [1000,6]], dtype=tf.float32)  # (4, 2)
temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)
```
- 여기서 주목할 점은 Query에 해당하는 temp_q의 값 [0, 10, 0]은 Key에 해당하는 temp_k의 두번째 값 [0, 10, 0]과 일치함. 그렇다면 어텐션 분포와 어텐션 값은?

```
# 함수 실행
temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)
print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)
print(temp_out) # 어텐션 값
```
  
```
tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)
tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)
```

- Query는 4개의 Key값 중 두번째 값과 일치하므로 어텐션 분포는 [0, 1, 0, 0]의 값을 가지며 결과적으로 Value의 두번째 값인 [10, 0]이 출력되는 것을 확인할 수 있음.
- 이번에는 Query의 값만 다른 값으로 바꿔보고 함수를 실행
  - Query값 [0, 0, 10]은 Key의 세번째 값과, 네번째 값 두 개의 값 모두와 일치하는 값

```
temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)
temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)
print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)
print(temp_out) # 어텐션 값
```
```
tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)
tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)
```
- Query의 값은 Key의 세번째 값과 네번째 값 두 개의 값과 모두 유사하다는 의미에서 어텐션 분포는 [0, 0, 0.5, 0.5]의 값을 가짐.
- 결과적으로 나오는 값 [550, 5.5]는 Value의 세번째 값 [100, 5]에 0.5를 곱한 값과 네번째 값 [1000, 6]에 0.5를 곱한 값의 원소별 합

-  이번에는 하나가 아닌 3개의 Query의 값을 함수의 입력으로 사용
```
temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)
temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)
print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)
print(temp_out) # 어텐션 값
```
```
tf.Tensor(
[[0.  0.  0.5 0.5]
 [0.  1.  0.  0. ]
 [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)
tf.Tensor(
[[550.    5.5]
 [ 10.    0. ]
 [  5.5   0. ]], shape=(3, 2), dtype=float32)
```

6) 멀티 헤드 어텐션(Multi-head Attention)
- 이제 num_heads의 의미와 왜 dmodel의 차원을 가진 단어 벡터를 가지고 어텐션을 하지 않고 차원을 축소시킨 벡터로 어텐션을 수행하였는지 이해해 볼 것

![image](https://user-images.githubusercontent.com/42113942/131239639-d8a9f0c6-6300-4c21-ab3b-200d164251a7.png)

- 트랜스포머 연구진은 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이라고 판단함.
  - 그리스로마신화에는 머리가 여러 개인 괴물 히드라나 케로베로스가 나옴. 이 괴물들의 특징은 머리가 여러 개이기 때문에 여러 시점에서 상대방을 볼 수 있다는 것. 이렇게 되면 시각에서 놓치는 게 별로 없을테니까 이런 괴물들에게 기습을 하는 것이 굉장히 힘이 들 것.
  - 멀티 헤드 어텐션도 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집하겠다는 것.

- '그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.'
  - 단어 그것(it)이 쿼리였다면 즉, it에 대한 Q벡터로부터 다른 단어와의 연관도를 구하였을 때 첫번째 어텐션 헤드는 '그것(it)'과 '동물(animal)'의 연관도를 높게 본다면, 두번째 어텐션 헤드는 '그것(it)'과 '피곤하였기 때문이다(tired)'의 연관도를 높게 볼 수 있음. 각 어텐션 헤드는 전부 다른 시각에서 보고있기 때문.
  
![image](https://user-images.githubusercontent.com/42113942/131239677-095bf4e0-bb95-4cf0-aaea-0774f9250593.png)

- 병렬 어텐션을 모두 수행하였다면 모든 어텐션 헤드를 연결(concatenate)함.
  - 모두 연결된 어텐션 헤드 행렬의 크기 = (seq_len, dmodel)

- 아래의 그림에서는 책의 지면상의 한계로 다시 dmodel를 4차원으로 표현함.
  
![image](https://user-images.githubusercontent.com/42113942/131239693-7f565d18-f99c-4c50-b0e1-52e3a9df3b15.png)

- 어텐션 헤드를 모두 연결한 행렬은 또 다른 가중치 행렬 Wo을 곱하게 되는데, 이렇게 나온 결과 행렬이 멀티-헤드 어텐션의 최종 결과물임.
- 위의 그림은 어텐션 헤드를 모두 연결한 행렬이 가중치 행렬 Wo과 곱해지는 과정을 보여줌. 이때 결과물인 멀티-헤드 어텐션 행렬은 인코더의 입력이었던 문장 행렬의 (seq_len, dmodel) 크기와 동일함.

- 다시 말해 인코더의 첫번째 서브층인 멀티-헤드 어텐션 단계를 끝마쳤을 때, 인코더의 입력으로 들어왔던 행렬의 크기가 아직 유지되고 있음을 기억할 것
  -  첫번째 서브층인 멀티-헤드 어텐션과 두번째 서브층인 포지션 와이즈 피드 포워드 신경망을 지나면서 인코더의 입력으로 들어올 때의 행렬의 크기는 계속 유지되어야 함.
  - 트랜스포머는 다수의 인코더를 쌓은 형태인데(논문에서는 인코더가 6개), 인코더에서의 입력의 크기가 출력에서도 동일 크기로 계속 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있기 때문
  
7) 멀티 헤드 어텐션(Multi-head Attention) 구현하기
- 멀티 헤드 어텐션에서는 크게 두 종류의 가중치 행렬이 나옴.
  - Q, K, V 행렬을 만들기 위한 가중치 행렬인 WQ, WK, WV 행렬과 바로 어텐션 헤드들을 연결(concatenation) 후에 곱해주는 WO 행렬
  - 가중치 행렬을 곱하는 것을 구현 상에서는 입력을 밀집층(Dense layer)를 지나게 함.
  
```
Dense(units)
```  

- 멀티 헤드 어텐션의 구현은 크게 다섯 가지 파트로 구성됨.
  1. WQ, WK, WV에 해당하는 d_model 크기의 밀집층(Dense layer)을 지나게한다.
  2. 지정된 헤드 수(num_heads)만큼 나눈다(split).
  3. 스케일드 닷 프로덕트 어텐션.
  4. 나눠졌던 헤드들을 연결(concatenatetion)한다.
  5. WO에 해당하는 밀집층을 지나게 한다.
  
``` 
class MultiHeadAttention(tf.keras.layers.Layer):

  def __init__(self, d_model, num_heads, name="multi_head_attention"):
    super(MultiHeadAttention, self).__init__(name=name)
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    # d_model을 num_heads로 나눈 값.
    # 논문 기준 : 64
    self.depth = d_model // self.num_heads

    # WQ, WK, WV에 해당하는 밀집층 정의
    self.query_dense = tf.keras.layers.Dense(units=d_model)
    self.key_dense = tf.keras.layers.Dense(units=d_model)
    self.value_dense = tf.keras.layers.Dense(units=d_model)

    # WO에 해당하는 밀집층 정의
    self.dense = tf.keras.layers.Dense(units=d_model)

  # num_heads 개수만큼 q, k, v를 split하는 함수
  def split_heads(self, inputs, batch_size):
    inputs = tf.reshape(
        inputs, shape=(batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(inputs, perm=[0, 2, 1, 3])

  def call(self, inputs):
    query, key, value, mask = inputs['query'], inputs['key'], inputs[
        'value'], inputs['mask']
    batch_size = tf.shape(query)[0]

    # 1. WQ, WK, WV에 해당하는 밀집층 지나기
    # q : (batch_size, query의 문장 길이, d_model)
    # k : (batch_size, key의 문장 길이, d_model)
    # v : (batch_size, value의 문장 길이, d_model)
    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.
    query = self.query_dense(query)
    key = self.key_dense(key)
    value = self.value_dense(value)

    # 2. 헤드 나누기
    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)
    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)
    query = self.split_heads(query, batch_size)
    key = self.split_heads(key, batch_size)
    value = self.split_heads(value, batch_size)

    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.
    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)
    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)
    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)
    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

    # 4. 헤드 연결(concatenate)하기
    # (batch_size, query의 문장 길이, d_model)
    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model))

    # 5. WO에 해당하는 밀집층 지나기
    # (batch_size, query의 문장 길이, d_model)
    outputs = self.dense(concat_attention)

    return outputs
```  
  
8) 패딩 마스크(Padding Mask)
- 앞서 구현한 스케일드 닷 프로덕트 어텐션 함수 내부를 보면 mask라는 값을 인자로 받아서, 이 mask값에다가 -1e9라는 아주 작은 음수값을 곱한 후 어텐션 스코어 행렬에 더해주고 있음.
 
```  
def scaled_dot_product_attention(query, key, value, mask):
... 중략 ...
    logits += (mask * -1e9) # 어텐션 스코어 행렬인 logits에 mask*-1e9 값을 더해주고 있다.
... 중략 ...
```  
  
- 이는 입력 문장에 <PAD> 토큰이 있을 경우 어텐션에서 사실상 제외하기 위한 연산임.
- 아래는 <PAD>가 포함된 입력 문장에 대해서 어텐션을 수행하고 어텐션 스코어 행렬을 얻는 과정
  
  ![image](https://user-images.githubusercontent.com/42113942/131239834-108657a6-f215-463c-a356-64e36b9ee569.png)
  
- 그런데 사실 단어 <PAD>의 경우에는 실질적인 의미를 가진 단어가 아님. 그래서 트랜스포머에서는 Key의 경우에 <PAD> 토큰이 존재한다면 이에 대해서는 유사도를 구하지 않도록 마스킹(Masking)을 해주기로 함.
  - 마스킹: 어텐션에서 제외하기 위해 값을 가린다는 의미
  
- 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query이고, 열에 해당하는 문장은 Key. 그리고 Key에 <PAD>가 있는 경우에는 해당 열 전체를 마스킹함.
  
  ![image](https://user-images.githubusercontent.com/42113942/131239856-bef6e07c-4d2a-49a1-9c18-a3ab28ced690.png)

- 마스킹을 하는 방법은 어텐션 스코어 행렬의 마스킹 위치에 매우 작은 음수값을 넣어주는 것. 
  - 여기서 매우 작은 음수값이라는 것은 -1,000,000,000과 같은 -무한대에 가까운 수라는 의미
  - 어텐션 스코어 행렬이 소프트맥스 함수를 지난 후에는 해당 위치의 값은 0에 굉장히 가까운 값이 되어 단어 간 유사도를 구하는 일에 <PAD> 토큰이 반영되지 않게 됨.
  
  ![image](https://user-images.githubusercontent.com/42113942/131239868-00de8963-9a3d-46b4-9ac6-c5267fd36981.png)
  
- 패딩 마스크를 구현하는 방법은 입력된 정수 시퀀스에서 패딩 토큰의 인덱스인지, 아닌지를 판별하는 함수를 구현하는 것. 
- 아래의 함수는 정수 시퀀스에서 0인 경우에는 1로 변환하고, 그렇지 않은 경우에는 0으로 변환하는 함수

``` 
def create_padding_mask(x):
  mask = tf.cast(tf.math.equal(x, 0), tf.float32)
  # (batch_size, 1, 1, key의 문장 길이)
  return mask[:, tf.newaxis, tf.newaxis, :]
``` 
- 임의의 정수 시퀀스 입력을 넣어서 어떻게 변환되는지 살펴봄.
``` 
print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))
```
```
tf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)
```
  
- 위 벡터를 통해서 1의 값을 가진 위치의 열을 어텐션 스코어 행렬에서 마스킹하는 용도로 사용할 수 있음.
  - 위 벡터를 스케일드 닷 프로덕트 어텐션의 인자로 전달하면, 스케일드 닷 프로덕트 어텐션에서는 위 벡터에다가 매우 작은 음수값인 -1e9를 곱하고, 이를 행렬에 더해주어 해당 열을 전부 마스킹하게 되는 것

- 지금까지 첫번째 서브층인 멀티 헤드 어텐션을 구현함.
- 앞서 인코더는 두 개의 서브 서브층(sublayer)으로 나뉘어진다고 언급한 적이 있는데, 이제 두번째 서브층인 포지션-와이즈 피드 포워드 신경망에 대해서 알아봄.
  
## 17.1.8. 포지션-와이즈 피드 포워드 신경망(Position-wise FFNN)
- 지금은 인코더를 설명하고 있지만, 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층임.
- 포지션-와이즈 FFNN는 쉽게 말하면 완전 연결 FFNN(Fully-connected FFNN)이라고 해석할 수 있음.
- 아래는 포지션 와이즈 FFNN의 수식
  
  ![image](https://user-images.githubusercontent.com/42113942/131239953-70c1f41c-74e9-47c5-8d20-4c12a5dba146.png)

- 식을 그림으로 표현하면 아래와 같음.
  
  ![image](https://user-images.githubusercontent.com/42113942/131239963-2dc298c2-100c-43ce-80b2-39ddc6edb56f.png)

  - x는 앞서 멀티 헤드 어텐션의 결과로 나온 (seq_len, dmodel)의 크기를 가지는 행렬
  - 가중치 행렬 W1은 (dmodel, dff)의 크기를 가지고, 가중치 행렬 W2는 (dff, dmodel)의 크기를 가짐.
  - 논문에서 은닉층의 크기 dff는 앞서 하이퍼파라미터를 정의할 때 언급했듯이 2,048의 크기를 가짐.

- 여기서 매개변수 W1, b1, W2, b2는 하나의 인코더 층 내에서는 다른 문장, 다른 단어들마다 정확하게 동일하게 사용되지만 인코더 층마다는 다른 값을 가짐.
  
  ![image](https://user-images.githubusercontent.com/42113942/131240015-c9547a15-4221-4f69-9b33-cc9dd57c3885.png)

- 위의 그림에서 좌측은 인코더의 입력을 벡터 단위로 봤을 때, 각 벡터들이 멀티 헤드 어텐션 층이라는 인코더 내 첫번째 서브 층을 지나 FFNN을 통과하는 것을 보여줌. 이는 두번째 서브층인 Position-wise FFNN을 의미. 
- 물론, 실제로는 그림의 우측과 같이 행렬로 연산되는데, 두번째 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 (seq_len, dmodel)의 크기가 보존되고 있음.
-  하나의 인코더 층을 지난 이 행렬은 다음 인코더 층으로 전달되고, 다음 층에서도 동일한 인코더 연산이 반복됨.
  
- 이를 구현하면 다음과 같음.

```
# 다음의 코드는 인코더와 디코더 내부에서 사용할 예정입니다.
  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)
```
  
## 17.1.9. 잔차 연결(Residual connection)과 층 정규화(Layer Normalization)
![image](https://user-images.githubusercontent.com/42113942/131240065-7ebf6835-e0da-4118-ab0e-c5a82df20829.png)

- 트랜스포머에서는 이러한 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 기법이 있는데, 바로 Add & Norm임.
  - 더 정확히는 잔차 연결(residual connection)과 층 정규화(layer normalization)를 의미함.
  
- 위의 그림은 앞서 Position-wise FFNN를 설명할 때 사용한 앞선 그림에서 화살표와 Add & Norm(잔차 연결과 정규화 과정)을 추가한 그림
  - 추가된 화살표들은 서브층 이전의 입력에서 시작되어 서브층의 출력 부분을 향하고 있는 것에 주목할 것. 추가된 화살표가 어떤 의미를 갖고 있는지는 잔차 연결과 층 정규화를 배우고 나면 이해할 수 있음.
  
1) 잔차 연결(Residual connection)
- 잔차 연결(residual connection)의 의미를 이해하기 위해서 어떤 함수 H(x)에 대한 이야기를 해볼 것임.
  
  ![image](https://user-images.githubusercontent.com/42113942/131240110-346eddf2-acfa-4ac8-a7de-37c761eb1fb1.png)

  ![image](https://user-images.githubusercontent.com/42113942/131240112-0bbdd4ec-46e5-49ff-a34e-e6a58c645488.png)

- 위 그림은 입력 x와 x에 대한 어떤 함수 F(x)의 값을 더한 함수 H(x)의 구조를 보여줌.
  - F(x)가 트랜스포머의 서브층
  - 다시 말해 잔차 연결은 서브층의 입력과 출력을 더하는 것임.
  
- 이를 식으로 표현하면 x + Sublayer(x)라고 할 수 있음.
  
- 가령, 서브층이 멀티 헤드 어텐션이었다면 잔차 연결 연산은 다음과 같음.
  ![image](https://user-images.githubusercontent.com/42113942/131240156-ef6f3ce6-ed95-450b-866c-e893d2de8b60.png)

  ![image](https://user-images.githubusercontent.com/42113942/131240157-3273a407-32c2-478a-92c3-3f9a23296f65.png)

- 위 그림은 멀티 헤드 어텐션의 입력과 멀티 헤드 어텐션의 결과가 더해지는 과정을 보여줌.
- 관련 논문 : https://arxiv.org/pdf/1512.03385.pdf
  
2) 층 정규화(Layer Normalization)
- 잔차 연결의 입력을 x, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 LN이라고 하였을 때, 잔차 연결 후 층 정규화 연산을 수식으로 표현하자면 다음과 같음.
  
 ![image](https://user-images.githubusercontent.com/42113942/131240174-38d0dcdc-f5e0-4d3d-a660-0a4c36479d56.png)

- 층 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움. 
  - 여기서 텐서의 마지막 차원이란 것은 트랜스포머에서는 dmodel 차원을 의미합니다. 
- 아래 그림은 dmodel 차원의 방향을 화살표로 표현함.
  
![image](https://user-images.githubusercontent.com/42113942/131240205-304175c1-71ce-446e-bb33-bb9da6a6e741.png)

- 층 정규화를 위해서 우선, 화살표 방향으로 각각 평균과 분산을 구함. 각 화살표 방향의 벡터 xi
  
![image](https://user-images.githubusercontent.com/42113942/131240211-4b5e80d1-92c5-46a6-944b-982a73eb5b3c.png)

- 층 정규화를 수행한 후에는 벡터 xi는 lni라는 벡터로 정규화가 됨.

![image](https://user-images.githubusercontent.com/42113942/131240224-4b22e844-11b9-4095-86cc-e1fdb3eacac5.png)

- 이제 층 정규화의 수식을 알아봄.
- 층 정규화의 두 가지 과정
  - 첫번째는 평균과 분산을 통한 정규화, 두번째는 감마와 베타를 도입하는 것
    - 우선 평균과 분산을 통해 xi를 정규화
    - xi는 벡터인 반면, 평균과 분산은 스칼라
    - 벡터 xi의 각 차원을 k라고 하였을 때, xi,k는 다음의 수식과 같이 정규화할 수 있으.ㅁ
    - 다시 말해 벡터 xi의 각 k차원의 값이 다음과 같이 정규화됨.
  
    ![image](https://user-images.githubusercontent.com/42113942/131240335-6ca1c8bb-71db-41df-aa9e-0199168f7b81.png)

- 입실론은 분모가 0이 되는 것을 방지하는 값
  
- 이제 감마와 베타라는 벡터를 준비함. 단, 이들의 초기값은 각각 1과 0임
  ![image](https://user-images.githubusercontent.com/42113942/131240349-2c84c6f7-83fd-4db7-a49d-d088756ddcba.png)
![image](https://user-images.githubusercontent.com/42113942/131240352-72c2e687-6340-4f30-873d-f653179caad6.png)

- 감마와 벡터를 도입한 층 정규화의 최종 수식은 다음과 같음.
  ![image](https://user-images.githubusercontent.com/42113942/131240382-1a527153-5eac-4a91-8424-9cdb097b4ca0.png)

- 관련 논문 : https://arxiv.org/pdf/1607.06450.pdf
- 케라스에서는 LayerNormalization()를 이미 제공하고 있으므로, 이를 가져와 사용함.
  
## 17.1.10. 인코더 구현하기
- 지금까지 배운 내용을 바탕으로 인코더를 구현한 코드는 다음과 같음.
```
def encoder_layer(dff, d_model, num_heads, dropout, name="encoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")

  # 인코더는 패딩 마스크 사용
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)
  attention = MultiHeadAttention(
      d_model, num_heads, name="attention")({
          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V
          'mask': padding_mask # 패딩 마스크 사용
      })

  # 드롭아웃 + 잔차 연결과 층 정규화
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  attention = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(inputs + attention)

  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)
  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # 드롭아웃 + 잔차 연결과 층 정규화
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention + outputs)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
```
  
- 우선 인코더의 입력으로 들어가는 문장에는 패딩이 있을 수 있으므로, 어텐션 시 패딩 토큰을 제외하도록 패딩 마스크를 사용함.
  - 이는 MultiHeadAttention 함수의 mask의 인자값으로 padding_mask가 들어가는 이유임.
- 인코더는 총 두 개의 서브층으로 이루어지는데, 멀티 헤드 어텐션과 피드 포워드 신경망
- 각 서브층 이후에는 드롭 아웃, 잔차 연결과 층 정규화가 수행됨.
  
- 위 코드는 하나의 인코더 블록. 즉, 하나의 인코더 층을 구현하는 코드임. 물론, 실제로 트랜스포머는 num_layers 개수만큼의 인코더 층을 사용하므로 이를 여러번 쌓는 코드를 별도 구현해줄 필요가 있음.
  
## 17.1.11. 인코더 쌓기
- 인코더 층을 num_layers개만큼 쌓고, 마지막 인코더 층에서 얻는 (seq_len, d_model) 크기의 행렬을 디코더로 보내주므로서 트랜스포머 인코더의 인코딩 연산이 끝나게 됨.
- 아래의 코드는 인코더 층을 num_layers개만큼 쌓는 코드
```
def encoder(vocab_size, num_layers, dff,
            d_model, num_heads, dropout,
            name="encoder"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")

  # 인코더는 패딩 마스크 사용
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # 포지셔널 인코딩 + 드롭아웃
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)
  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  # 인코더를 num_layers개 쌓기
  for i in range(num_layers):
    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,
        dropout=dropout, name="encoder_layer_{}".format(i),
    )([outputs, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
```
  
